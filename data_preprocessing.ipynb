{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HK-Laptop-V639\\Documents\\GitHub\\696\\env696\\Scripts\\python.exe\n",
      "C:\\Users\\HK-Laptop-V639\\AppData\\Local\\Programs\\Python\\Python312\\python.exe\n",
      "C:\\Users\\HK-Laptop-V639\\AppData\\Local\\Microsoft\\WindowsApps\\python.exe\n"
     ]
    }
   ],
   "source": [
    "!where python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this code to make sure you install all the required libraries\n",
    "# be sure you are in virtual environment before install, otherwise it will overwrite your local environment\n",
    "\n",
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\", None)\n",
    "RANDOM_SEED = 123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_anime = pd.read_csv(\"assets/anime.csv\")\n",
    "df_manga = pd.read_csv(\"assets/manga.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((24985, 39), (64833, 30))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_anime.shape, df_manga.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prepocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some short synopsis contain no information about the story of manga/title. This will introduce noise to our model. Therefore, we decide to remove those rows with extremely short synopsis.\n",
    "Example:\n",
    "- Second season of Mao Zhi Ming.\n",
    "- The second season of Shen Lan Qi Yu Wushuang Zhu.\n",
    "- Recap episode of Hakyuu Houshin Engi.\n",
    "- Fifth Season of Bungou Stray Dogs\n",
    "- 1-3. Ba_ku\\n4-5. Mephisto\n",
    "- An absurd film by Kuri Youji.\n",
    "- Included one-shot:\\nBougainvillea\n",
    "- A collection of oneshots by Nishida Higashi.\n",
    "- A movie adaptation of the TV series.\n",
    "- Short film by Kurosaka Keita.\n",
    "- Special episodes added to DVDs and Blu-rays.\n",
    "- Movie based on the 1996 TV anime with an original plot.\n",
    "- Third season of Yuan Long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaned anime shape:  (8862, 21)\n",
      "cleaned manga shape:  (15447, 20)\n"
     ]
    }
   ],
   "source": [
    "def data_cleaning(input_anime, input_manga):\n",
    "    df_anime = input_anime.copy()\n",
    "    df_manga = input_manga.copy()\n",
    "\n",
    "    # remove unnecessary columns\n",
    "    df_anime = df_anime.drop(columns=['anime_id', 'total_duration', 'start_year', 'start_season', 'rating', 'main_picture', 'url', 'trailer_url', 'background', 'created_at', 'updated_at', 'episode_duration', 'broadcast_day', 'broadcast_time', 'licensors', 'title_synonyms', 'real_start_date', 'real_end_date'])\n",
    "    df_manga = df_manga.drop(columns=['manga_id', 'main_picture', 'url', 'background', 'created_at_before', 'updated_at', 'title_synonyms', 'volumes', 'real_start_date', 'real_end_date'])\n",
    "\n",
    "    # remove rows that are null in 'synopsis' and 'title', which are crucial for our project\n",
    "    df_anime.dropna(subset=['title', 'synopsis', 'title_english', 'title_japanese'], inplace=True)\n",
    "    df_manga.dropna(subset=['title', 'synopsis', 'title_english', 'title_japanese'], inplace=True)\n",
    "\n",
    "    # remove '(Sources:...)' from synopsis\n",
    "    df_anime['synopsis'] = df_anime['synopsis'].apply(lambda x: re.sub(r'\\(Source:.*\\)', '', x))\n",
    "    df_manga['synopsis'] = df_manga['synopsis'].apply(lambda x: re.sub(r'\\(Source:.*\\)', '', x))\n",
    "\n",
    "    # remove '[Written by ...]' from synopsis\n",
    "    df_anime['synopsis'] = df_anime['synopsis'].apply(lambda x: re.sub(r'\\[Written by.*\\]', '', x))\n",
    "    df_manga['synopsis'] = df_manga['synopsis'].apply(lambda x: re.sub(r'\\[Written by.*\\]', '', x))\n",
    "\n",
    "    # remove rows that have extreme short synopsis\n",
    "    df_anime = df_anime[df_anime['synopsis'].apply(lambda x: len(x) > 50)]\n",
    "    df_manga = df_manga[df_manga['synopsis'].apply(lambda x: len(x) > 50)]\n",
    "\n",
    "    print('cleaned anime shape: ', df_anime.shape)\n",
    "    print('cleaned manga shape: ', df_manga.shape)\n",
    "\n",
    "    return df_anime, df_manga\n",
    "\n",
    "df_anime_cleaned, df_manga_cleaned = data_cleaning(df_anime, df_manga)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anime extra columns: \n",
      " ['episodes', 'source', 'studios', 'producers']\n"
     ]
    }
   ],
   "source": [
    "print(\"anime extra columns: \\n\", [col for col in df_anime_cleaned.columns if not col in df_manga_cleaned.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manga extra columns: \n",
      " ['chapters', 'authors', 'serializations']\n"
     ]
    }
   ],
   "source": [
    "print(\"manga extra columns: \\n\", [col for col in df_manga_cleaned.columns if not col in df_anime_cleaned.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "common columns: \n",
      " ['title', 'type', 'score', 'scored_by', 'status', 'start_date', 'end_date', 'members', 'favorites', 'sfw', 'approved', 'genres', 'themes', 'demographics', 'synopsis', 'title_english', 'title_japanese']\n"
     ]
    }
   ],
   "source": [
    "print(\"common columns: \\n\", [col for col in df_anime_cleaned.columns if col in df_manga_cleaned.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "def extract_author_name(author_list):\n",
    "    try:\n",
    "        author_list = ast.literal_eval(author_list)\n",
    "        \n",
    "        # Extract first and last names of authors, ignoring the others\n",
    "        author_names = [f\"{author['first_name']} {author['last_name']}\".strip() for author in author_list if author['first_name'] or author['last_name']]\n",
    "        \n",
    "        # Join names for multiple authors\n",
    "        return ', '.join(author_names) if author_names else \"Missing\"\n",
    "    \n",
    "    except (ValueError, SyntaxError, KeyError, TypeError):\n",
    "        # Handle Missing data\n",
    "        return \"Missing\"\n",
    "\n",
    "# Apply the function to the 'authors' column\n",
    "def authors_extraction(input_manga):\n",
    "    df_manga = input_manga.copy()\n",
    "    df_manga['authors'] = df_manga['authors'].apply(extract_author_name)\n",
    "\n",
    "    return df_manga\n",
    "\n",
    "df_manga_extracted = authors_extraction(df_manga_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([\"[{'id': 1868, 'first_name': 'Kentarou', 'last_name': 'Miura', 'role': 'Story & Art'}, {'id': 49592, 'first_name': '', 'last_name': 'Studio Gaga', 'role': 'Art'}]\"],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_manga_cleaned['authors'].head(1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Kentarou Miura, Studio Gaga'], dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_manga_extracted['authors'].head(1).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extra columns alignment**:\n",
    "| anime columns | manga columns | strategy |\n",
    "| --- | --- | --- |\n",
    "| episodes | chapters | episodes/chapters |\n",
    "| source | NULL | impute const 'Missing' |\n",
    "| studios | authors | creators |\n",
    "| producers | serializations | production_source |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def columns_alignment(input_anime, input_manga):\n",
    "    df_anime = input_anime.copy()\n",
    "    df_manga = input_manga.copy()\n",
    "\n",
    "    # treat 'episodes' and 'chapters' the same, create null value for 'volume' in anime\n",
    "    df_anime.rename(columns={'episodes': 'episodes/chapters'}, inplace=True)\n",
    "    df_manga.rename(columns={'chapters': 'episodes/chapters'}, inplace=True)\n",
    "\n",
    "    # Combine studios and authors together to get creators columns\n",
    "    df_anime.rename(columns={'studios': 'creators'}, inplace=True)\n",
    "    df_manga.rename(columns={'authors': 'creators'}, inplace=True)\n",
    "\n",
    "    # Also for producers and serialization\n",
    "    df_anime.rename(columns={'producers': 'production_source'}, inplace=True)\n",
    "    df_manga.rename(columns={'serializations': 'production_source'}, inplace=True)\n",
    "\n",
    "    # To distinguish where the data from\n",
    "    df_anime['is_anime'] = 1\n",
    "    df_manga['is_anime'] = 0 \n",
    "    \n",
    "    return df_anime, df_manga\n",
    "\n",
    "df_anime_aligned, df_manga_aligned = columns_alignment(df_anime_cleaned, df_manga_extracted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>type</th>\n",
       "      <th>score</th>\n",
       "      <th>scored_by</th>\n",
       "      <th>status</th>\n",
       "      <th>episodes/chapters</th>\n",
       "      <th>start_date</th>\n",
       "      <th>end_date</th>\n",
       "      <th>source</th>\n",
       "      <th>members</th>\n",
       "      <th>favorites</th>\n",
       "      <th>sfw</th>\n",
       "      <th>approved</th>\n",
       "      <th>genres</th>\n",
       "      <th>themes</th>\n",
       "      <th>demographics</th>\n",
       "      <th>creators</th>\n",
       "      <th>production_source</th>\n",
       "      <th>synopsis</th>\n",
       "      <th>title_english</th>\n",
       "      <th>title_japanese</th>\n",
       "      <th>is_anime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fullmetal Alchemist: Brotherhood</td>\n",
       "      <td>tv</td>\n",
       "      <td>9.10</td>\n",
       "      <td>2037075</td>\n",
       "      <td>finished_airing</td>\n",
       "      <td>64.0</td>\n",
       "      <td>2009-04-05</td>\n",
       "      <td>2010-07-04</td>\n",
       "      <td>manga</td>\n",
       "      <td>3206028</td>\n",
       "      <td>219036</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>['Action', 'Adventure', 'Drama', 'Fantasy']</td>\n",
       "      <td>['Military']</td>\n",
       "      <td>['Shounen']</td>\n",
       "      <td>['Bones']</td>\n",
       "      <td>['Aniplex', 'Square Enix', 'Mainichi Broadcast...</td>\n",
       "      <td>After a horrific alchemy experiment goes wrong...</td>\n",
       "      <td>Fullmetal Alchemist: Brotherhood</td>\n",
       "      <td>鋼の錬金術師 FULLMETAL ALCHEMIST</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hunter x Hunter (2011)</td>\n",
       "      <td>tv</td>\n",
       "      <td>9.04</td>\n",
       "      <td>1671587</td>\n",
       "      <td>finished_airing</td>\n",
       "      <td>148.0</td>\n",
       "      <td>2011-10-02</td>\n",
       "      <td>2014-09-24</td>\n",
       "      <td>manga</td>\n",
       "      <td>2688079</td>\n",
       "      <td>202109</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>['Action', 'Adventure', 'Fantasy']</td>\n",
       "      <td>[]</td>\n",
       "      <td>['Shounen']</td>\n",
       "      <td>['Madhouse']</td>\n",
       "      <td>['VAP', 'Nippon Television Network', 'Shueisha']</td>\n",
       "      <td>Hunters devote themselves to accomplishing haz...</td>\n",
       "      <td>Hunter x Hunter</td>\n",
       "      <td>HUNTER×HUNTER（ハンター×ハンター）</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Shingeki no Kyojin Season 3 Part 2</td>\n",
       "      <td>tv</td>\n",
       "      <td>9.05</td>\n",
       "      <td>1491491</td>\n",
       "      <td>finished_airing</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2019-04-29</td>\n",
       "      <td>2019-07-01</td>\n",
       "      <td>manga</td>\n",
       "      <td>2133927</td>\n",
       "      <td>55644</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>['Action', 'Drama']</td>\n",
       "      <td>['Gore', 'Military', 'Survival']</td>\n",
       "      <td>['Shounen']</td>\n",
       "      <td>['Wit Studio']</td>\n",
       "      <td>['Production I.G', 'Dentsu', 'Mainichi Broadca...</td>\n",
       "      <td>Seeking to restore humanity's diminishing hope...</td>\n",
       "      <td>Attack on Titan Season 3 Part 2</td>\n",
       "      <td>進撃の巨人 Season3 Part.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Steins;Gate</td>\n",
       "      <td>tv</td>\n",
       "      <td>9.07</td>\n",
       "      <td>1348232</td>\n",
       "      <td>finished_airing</td>\n",
       "      <td>24.0</td>\n",
       "      <td>2011-04-06</td>\n",
       "      <td>2011-09-14</td>\n",
       "      <td>visual_novel</td>\n",
       "      <td>2463954</td>\n",
       "      <td>184312</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>['Drama', 'Sci-Fi', 'Suspense']</td>\n",
       "      <td>['Psychological', 'Time Travel']</td>\n",
       "      <td>[]</td>\n",
       "      <td>['White Fox']</td>\n",
       "      <td>['Frontier Works', 'Media Factory', 'Kadokawa ...</td>\n",
       "      <td>Eccentric scientist Rintarou Okabe has a never...</td>\n",
       "      <td>Steins;Gate</td>\n",
       "      <td>STEINS;GATE</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Koe no Katachi</td>\n",
       "      <td>movie</td>\n",
       "      <td>8.94</td>\n",
       "      <td>1540277</td>\n",
       "      <td>finished_airing</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2016-09-17</td>\n",
       "      <td>2016-09-17</td>\n",
       "      <td>manga</td>\n",
       "      <td>2218467</td>\n",
       "      <td>84124</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>['Award Winning', 'Drama']</td>\n",
       "      <td>['Romantic Subtext']</td>\n",
       "      <td>['Shounen']</td>\n",
       "      <td>['Kyoto Animation']</td>\n",
       "      <td>['Shochiku', 'Pony Canyon', 'Kodansha', 'ABC A...</td>\n",
       "      <td>As a wild youth, elementary school student Sho...</td>\n",
       "      <td>A Silent Voice</td>\n",
       "      <td>聲の形</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                title   type  score  scored_by  \\\n",
       "0    Fullmetal Alchemist: Brotherhood     tv   9.10    2037075   \n",
       "1              Hunter x Hunter (2011)     tv   9.04    1671587   \n",
       "2  Shingeki no Kyojin Season 3 Part 2     tv   9.05    1491491   \n",
       "3                         Steins;Gate     tv   9.07    1348232   \n",
       "4                      Koe no Katachi  movie   8.94    1540277   \n",
       "\n",
       "            status  episodes/chapters  start_date    end_date        source  \\\n",
       "0  finished_airing               64.0  2009-04-05  2010-07-04         manga   \n",
       "1  finished_airing              148.0  2011-10-02  2014-09-24         manga   \n",
       "2  finished_airing               10.0  2019-04-29  2019-07-01         manga   \n",
       "3  finished_airing               24.0  2011-04-06  2011-09-14  visual_novel   \n",
       "4  finished_airing                1.0  2016-09-17  2016-09-17         manga   \n",
       "\n",
       "   members  favorites   sfw  approved  \\\n",
       "0  3206028     219036  True      True   \n",
       "1  2688079     202109  True      True   \n",
       "2  2133927      55644  True      True   \n",
       "3  2463954     184312  True      True   \n",
       "4  2218467      84124  True      True   \n",
       "\n",
       "                                        genres  \\\n",
       "0  ['Action', 'Adventure', 'Drama', 'Fantasy']   \n",
       "1           ['Action', 'Adventure', 'Fantasy']   \n",
       "2                          ['Action', 'Drama']   \n",
       "3              ['Drama', 'Sci-Fi', 'Suspense']   \n",
       "4                   ['Award Winning', 'Drama']   \n",
       "\n",
       "                             themes demographics             creators  \\\n",
       "0                      ['Military']  ['Shounen']            ['Bones']   \n",
       "1                                []  ['Shounen']         ['Madhouse']   \n",
       "2  ['Gore', 'Military', 'Survival']  ['Shounen']       ['Wit Studio']   \n",
       "3  ['Psychological', 'Time Travel']           []        ['White Fox']   \n",
       "4              ['Romantic Subtext']  ['Shounen']  ['Kyoto Animation']   \n",
       "\n",
       "                                   production_source  \\\n",
       "0  ['Aniplex', 'Square Enix', 'Mainichi Broadcast...   \n",
       "1   ['VAP', 'Nippon Television Network', 'Shueisha']   \n",
       "2  ['Production I.G', 'Dentsu', 'Mainichi Broadca...   \n",
       "3  ['Frontier Works', 'Media Factory', 'Kadokawa ...   \n",
       "4  ['Shochiku', 'Pony Canyon', 'Kodansha', 'ABC A...   \n",
       "\n",
       "                                            synopsis  \\\n",
       "0  After a horrific alchemy experiment goes wrong...   \n",
       "1  Hunters devote themselves to accomplishing haz...   \n",
       "2  Seeking to restore humanity's diminishing hope...   \n",
       "3  Eccentric scientist Rintarou Okabe has a never...   \n",
       "4  As a wild youth, elementary school student Sho...   \n",
       "\n",
       "                      title_english              title_japanese  is_anime  \n",
       "0  Fullmetal Alchemist: Brotherhood  鋼の錬金術師 FULLMETAL ALCHEMIST         1  \n",
       "1                   Hunter x Hunter    HUNTER×HUNTER（ハンター×ハンター）         1  \n",
       "2   Attack on Titan Season 3 Part 2        進撃の巨人 Season3 Part.2         1  \n",
       "3                       Steins;Gate                 STEINS;GATE         1  \n",
       "4                    A Silent Voice                         聲の形         1  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_full = pd.concat([df_anime_aligned, df_manga_aligned], ignore_index=True)\n",
    "\n",
    "df_full.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract year and month\n",
    "def extract_year_and_month(date):\n",
    "    try:\n",
    "        # Convert the date string to a datetime object\n",
    "        datetime_date = pd.to_datetime(date, errors='raise')\n",
    "        # Extract year and month\n",
    "        return datetime_date.year, datetime_date.month\n",
    "    except:\n",
    "        # return NaN if fails\n",
    "        return np.nan, np.nan\n",
    "    \n",
    "# Apply the function to the 'start_date' column\n",
    "df_full[['start_year', 'start_month']] = df_full['start_date'].apply(lambda x: pd.Series(extract_year_and_month(x)))\n",
    "\n",
    "# Apply the function to the 'end_date' column\n",
    "df_full[['end_year', 'end_month']] = df_full['end_date'].apply(lambda x: pd.Series(extract_year_and_month(x)))\n",
    "\n",
    "#remove useless columns\n",
    "df_full = df_full.drop(columns=['start_date', 'end_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start_year</th>\n",
       "      <th>end_year</th>\n",
       "      <th>start_season</th>\n",
       "      <th>end_season</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2009</td>\n",
       "      <td>2010</td>\n",
       "      <td>Spring</td>\n",
       "      <td>Summer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011</td>\n",
       "      <td>2014</td>\n",
       "      <td>Autumn</td>\n",
       "      <td>Summer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019</td>\n",
       "      <td>2019</td>\n",
       "      <td>Spring</td>\n",
       "      <td>Summer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2011</td>\n",
       "      <td>2011</td>\n",
       "      <td>Spring</td>\n",
       "      <td>Summer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016</td>\n",
       "      <td>2016</td>\n",
       "      <td>Summer</td>\n",
       "      <td>Summer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   start_year  end_year start_season end_season\n",
       "0        2009      2010       Spring     Summer\n",
       "1        2011      2014       Autumn     Summer\n",
       "2        2019      2019       Spring     Summer\n",
       "3        2011      2011       Spring     Summer\n",
       "4        2016      2016       Summer     Summer"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to transform Start or End month into season refer to Events of Anime\n",
    "def month_to_season(month):\n",
    "    # If the data is in range, return corresponding Season of events of Anime\n",
    "    if month in [1, 2, 3]:\n",
    "        return 'Winter'\n",
    "    elif month in [4, 5, 6]:\n",
    "        return 'Spring'\n",
    "    elif month in [7, 8, 9]:\n",
    "        return 'Summer'\n",
    "    elif month in [10, 11, 12]:\n",
    "        return 'Autumn'\n",
    "    else:\n",
    "        return np.nan  # Handle unexpected cases, though this shouldn't occur with valid months\n",
    "\n",
    "# Apply the function to transform the month value to season categories\n",
    "df_full['start_season'] = df_full['start_month'].apply(month_to_season)\n",
    "df_full['end_season'] = df_full['end_month'].apply(month_to_season)\n",
    "\n",
    "# convert year to int\n",
    "df_full['start_year'] = df_full['start_year'].astype('Int64')\n",
    "df_full['end_year'] = df_full['end_year'].astype('Int64')\n",
    "\n",
    "# remove useless columns\n",
    "df_full = df_full.drop(columns=['start_month', 'end_month'])\n",
    "\n",
    "# Check for Year and Season feature\n",
    "df_full[['start_year', 'end_year', 'start_season', 'end_season']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in train set:  19447\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(df_full, test_size=0.2, random_state=RANDOM_SEED)\n",
    "print('Number of rows in train set: ', len(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing data in Training Set:\n",
      "\n",
      "title                    0\n",
      "type                     6\n",
      "score                 5962\n",
      "scored_by                0\n",
      "status                   0\n",
      "episodes/chapters     3898\n",
      "source               12984\n",
      "members                  0\n",
      "favorites                0\n",
      "sfw                      0\n",
      "approved                 0\n",
      "genres                   0\n",
      "themes                   0\n",
      "demographics             0\n",
      "creators                 0\n",
      "production_source        0\n",
      "synopsis                 0\n",
      "title_english            0\n",
      "title_japanese           0\n",
      "is_anime                 0\n",
      "start_year             324\n",
      "end_year              3490\n",
      "start_season           324\n",
      "end_season            3490\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# check for missing data \n",
    "print(\"Number of missing data in Training Set:\\n\")\n",
    "print(train.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing data in Testing Set:\n",
      "\n",
      "title                   0\n",
      "type                    2\n",
      "score                1496\n",
      "scored_by               0\n",
      "status                  0\n",
      "episodes/chapters     959\n",
      "source               3237\n",
      "members                 0\n",
      "favorites               0\n",
      "sfw                     0\n",
      "approved                0\n",
      "genres                  0\n",
      "themes                  0\n",
      "demographics            0\n",
      "creators                0\n",
      "production_source       0\n",
      "synopsis                0\n",
      "title_english           0\n",
      "title_japanese          0\n",
      "is_anime                0\n",
      "start_year             79\n",
      "end_year              863\n",
      "start_season           79\n",
      "end_season            863\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# check for missing data \n",
    "print(\"Number of missing data in Testing Set:\\n\")\n",
    "print(test.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle Missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'Toyama Kankou Anime Project'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m knn_imputer \u001b[38;5;241m=\u001b[39m KNNImputer(n_neighbors\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Impute numerical columns\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m train \u001b[38;5;241m=\u001b[39m \u001b[43mknn_imputer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Check for result\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of missing data in Training Set:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\HK-Laptop-V639\\Documents\\GitHub\\696\\env696\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:316\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 316\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    317\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    318\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    319\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    320\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    321\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    322\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\HK-Laptop-V639\\Documents\\GitHub\\696\\env696\\Lib\\site-packages\\sklearn\\base.py:1098\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m   1083\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1084\u001b[0m             (\n\u001b[0;32m   1085\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis object (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) has a `transform`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1093\u001b[0m             \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[0;32m   1094\u001b[0m         )\n\u001b[0;32m   1096\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1097\u001b[0m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[1;32m-> 1098\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1100\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[0;32m   1101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32mc:\\Users\\HK-Laptop-V639\\Documents\\GitHub\\696\\env696\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\HK-Laptop-V639\\Documents\\GitHub\\696\\env696\\Lib\\site-packages\\sklearn\\impute\\_knn.py:230\u001b[0m, in \u001b[0;36mKNNImputer.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    227\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    228\u001b[0m     force_all_finite \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 230\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    231\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    232\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    233\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    234\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_X \u001b[38;5;241m=\u001b[39m X\n\u001b[0;32m    239\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mask_fit_X \u001b[38;5;241m=\u001b[39m _get_mask(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmissing_values)\n",
      "File \u001b[1;32mc:\\Users\\HK-Laptop-V639\\Documents\\GitHub\\696\\env696\\Lib\\site-packages\\sklearn\\base.py:633\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    631\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 633\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[0;32m    635\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[1;32mc:\\Users\\HK-Laptop-V639\\Documents\\GitHub\\696\\env696\\Lib\\site-packages\\sklearn\\utils\\validation.py:929\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    924\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pandas_requires_conversion:\n\u001b[0;32m    925\u001b[0m     \u001b[38;5;66;03m# pandas dataframe requires conversion earlier to handle extension dtypes with\u001b[39;00m\n\u001b[0;32m    926\u001b[0m     \u001b[38;5;66;03m# nans\u001b[39;00m\n\u001b[0;32m    927\u001b[0m     \u001b[38;5;66;03m# Use the original dtype for conversion if dtype is None\u001b[39;00m\n\u001b[0;32m    928\u001b[0m     new_dtype \u001b[38;5;241m=\u001b[39m dtype_orig \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m dtype\n\u001b[1;32m--> 929\u001b[0m     array \u001b[38;5;241m=\u001b[39m \u001b[43marray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_dtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    930\u001b[0m     \u001b[38;5;66;03m# Since we converted here, we do not need to convert again later\u001b[39;00m\n\u001b[0;32m    931\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\HK-Laptop-V639\\Documents\\GitHub\\696\\env696\\Lib\\site-packages\\pandas\\core\\generic.py:6643\u001b[0m, in \u001b[0;36mNDFrame.astype\u001b[1;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[0;32m   6637\u001b[0m     results \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m   6638\u001b[0m         ser\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39mcopy, errors\u001b[38;5;241m=\u001b[39merrors) \u001b[38;5;28;01mfor\u001b[39;00m _, ser \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m   6639\u001b[0m     ]\n\u001b[0;32m   6641\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6642\u001b[0m     \u001b[38;5;66;03m# else, only a single dtype is given\u001b[39;00m\n\u001b[1;32m-> 6643\u001b[0m     new_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6644\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor_from_mgr(new_data, axes\u001b[38;5;241m=\u001b[39mnew_data\u001b[38;5;241m.\u001b[39maxes)\n\u001b[0;32m   6645\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mastype\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\HK-Laptop-V639\\Documents\\GitHub\\696\\env696\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:430\u001b[0m, in \u001b[0;36mBaseBlockManager.astype\u001b[1;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[0;32m    427\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m using_copy_on_write():\n\u001b[0;32m    428\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 430\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    431\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mastype\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    432\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    433\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    434\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    435\u001b[0m \u001b[43m    \u001b[49m\u001b[43musing_cow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musing_copy_on_write\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    436\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\HK-Laptop-V639\\Documents\\GitHub\\696\\env696\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:363\u001b[0m, in \u001b[0;36mBaseBlockManager.apply\u001b[1;34m(self, f, align_keys, **kwargs)\u001b[0m\n\u001b[0;32m    361\u001b[0m         applied \u001b[38;5;241m=\u001b[39m b\u001b[38;5;241m.\u001b[39mapply(f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    362\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 363\u001b[0m         applied \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    364\u001b[0m     result_blocks \u001b[38;5;241m=\u001b[39m extend_blocks(applied, result_blocks)\n\u001b[0;32m    366\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mfrom_blocks(result_blocks, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes)\n",
      "File \u001b[1;32mc:\\Users\\HK-Laptop-V639\\Documents\\GitHub\\696\\env696\\Lib\\site-packages\\pandas\\core\\internals\\blocks.py:758\u001b[0m, in \u001b[0;36mBlock.astype\u001b[1;34m(self, dtype, copy, errors, using_cow, squeeze)\u001b[0m\n\u001b[0;32m    755\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan not squeeze with more than one column.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    756\u001b[0m     values \u001b[38;5;241m=\u001b[39m values[\u001b[38;5;241m0\u001b[39m, :]  \u001b[38;5;66;03m# type: ignore[call-overload]\u001b[39;00m\n\u001b[1;32m--> 758\u001b[0m new_values \u001b[38;5;241m=\u001b[39m \u001b[43mastype_array_safe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    760\u001b[0m new_values \u001b[38;5;241m=\u001b[39m maybe_coerce_values(new_values)\n\u001b[0;32m    762\u001b[0m refs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\HK-Laptop-V639\\Documents\\GitHub\\696\\env696\\Lib\\site-packages\\pandas\\core\\dtypes\\astype.py:237\u001b[0m, in \u001b[0;36mastype_array_safe\u001b[1;34m(values, dtype, copy, errors)\u001b[0m\n\u001b[0;32m    234\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtype\u001b[38;5;241m.\u001b[39mnumpy_dtype\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 237\u001b[0m     new_values \u001b[38;5;241m=\u001b[39m \u001b[43mastype_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n\u001b[0;32m    239\u001b[0m     \u001b[38;5;66;03m# e.g. _astype_nansafe can fail on object-dtype of strings\u001b[39;00m\n\u001b[0;32m    240\u001b[0m     \u001b[38;5;66;03m#  trying to convert to float\u001b[39;00m\n\u001b[0;32m    241\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\HK-Laptop-V639\\Documents\\GitHub\\696\\env696\\Lib\\site-packages\\pandas\\core\\dtypes\\astype.py:182\u001b[0m, in \u001b[0;36mastype_array\u001b[1;34m(values, dtype, copy)\u001b[0m\n\u001b[0;32m    179\u001b[0m     values \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39mcopy)\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 182\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[43m_astype_nansafe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;66;03m# in pandas we don't store numpy str dtypes, so convert to object\u001b[39;00m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, np\u001b[38;5;241m.\u001b[39mdtype) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(values\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mtype, \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\HK-Laptop-V639\\Documents\\GitHub\\696\\env696\\Lib\\site-packages\\pandas\\core\\dtypes\\astype.py:133\u001b[0m, in \u001b[0;36m_astype_nansafe\u001b[1;34m(arr, dtype, copy, skipna)\u001b[0m\n\u001b[0;32m    129\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mor\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m dtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m:\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;66;03m# Explicit copy, or required since NumPy can't view from / to object.\u001b[39;00m\n\u001b[1;32m--> 133\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39mcopy)\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'Toyama Kankou Anime Project'"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# Non-numerical columns\n",
    "Non_Numerical = ['type', 'source', 'start_season', 'end_season']\n",
    "\n",
    "# Create an imputer for non-numerical data filling with 'Missing'\n",
    "categorical_imputer = SimpleImputer(strategy='constant', fill_value='Missing')\n",
    "\n",
    "# Impute non-numerical columns\n",
    "train[Non_Numerical] = categorical_imputer.fit_transform(train[Non_Numerical])\n",
    "\n",
    "# Numerical columns\n",
    "Numerical = ['score', 'scored_by', 'episodes/chapters', 'members', 'favorites', 'start_year' , 'end_year']\n",
    "\n",
    "# Create an imputer for numerical data using KNNImputer\n",
    "knn_imputer = KNNImputer(n_neighbors=3)\n",
    "\n",
    "# Impute numerical columns\n",
    "train[Numerical] = knn_imputer.fit_transform(train[Numerical])\n",
    "\n",
    "# Check for result\n",
    "print(\"Number of missing data in Training Set:\\n\")\n",
    "print(train.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>type</th>\n",
       "      <th>score</th>\n",
       "      <th>scored_by</th>\n",
       "      <th>status</th>\n",
       "      <th>episodes/chapters</th>\n",
       "      <th>source</th>\n",
       "      <th>members</th>\n",
       "      <th>favorites</th>\n",
       "      <th>sfw</th>\n",
       "      <th>approved</th>\n",
       "      <th>genres</th>\n",
       "      <th>themes</th>\n",
       "      <th>demographics</th>\n",
       "      <th>creators</th>\n",
       "      <th>production_source</th>\n",
       "      <th>synopsis</th>\n",
       "      <th>title_english</th>\n",
       "      <th>title_japanese</th>\n",
       "      <th>is_anime</th>\n",
       "      <th>start_season</th>\n",
       "      <th>end_season</th>\n",
       "      <th>elapsed_start_time</th>\n",
       "      <th>elapsed_end_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13870</th>\n",
       "      <td>Souzai Saishuka no Isekai Ryokouki</td>\n",
       "      <td>manga</td>\n",
       "      <td>6.530000</td>\n",
       "      <td>-0.178503</td>\n",
       "      <td>currently_publishing</td>\n",
       "      <td>-0.285907</td>\n",
       "      <td>Missing</td>\n",
       "      <td>-0.185451</td>\n",
       "      <td>-0.111569</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>['Adventure', 'Fantasy']</td>\n",
       "      <td>['Isekai', 'Reincarnation']</td>\n",
       "      <td>[]</td>\n",
       "      <td>Tomozo, Masuo Kinoko</td>\n",
       "      <td>['AlphaPolis Web Manga']</td>\n",
       "      <td>Takeru Kamishiro is an ordinary middle aged sa...</td>\n",
       "      <td>A Gatherer's Adventure in Isekai</td>\n",
       "      <td>素材採取家の異世界旅行記</td>\n",
       "      <td>0</td>\n",
       "      <td>Autumn</td>\n",
       "      <td>Missing</td>\n",
       "      <td>100.0</td>\n",
       "      <td>92.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8825</th>\n",
       "      <td>Kingyo no Isshou</td>\n",
       "      <td>movie</td>\n",
       "      <td>6.583333</td>\n",
       "      <td>-0.202222</td>\n",
       "      <td>finished_airing</td>\n",
       "      <td>-0.462556</td>\n",
       "      <td>original</td>\n",
       "      <td>-0.236163</td>\n",
       "      <td>-0.114968</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>['Slice of Life']</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>An elementary school student named Mitsui won ...</td>\n",
       "      <td>Short and Happy Life of a Goldfish</td>\n",
       "      <td>金魚の一生</td>\n",
       "      <td>1</td>\n",
       "      <td>Winter</td>\n",
       "      <td>Winter</td>\n",
       "      <td>76.0</td>\n",
       "      <td>76.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6178</th>\n",
       "      <td>Takara-sagashi</td>\n",
       "      <td>movie</td>\n",
       "      <td>6.170000</td>\n",
       "      <td>-0.199299</td>\n",
       "      <td>finished_airing</td>\n",
       "      <td>-0.462556</td>\n",
       "      <td>picture_book</td>\n",
       "      <td>-0.224216</td>\n",
       "      <td>-0.114968</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>['Adventure', 'Fantasy']</td>\n",
       "      <td>[]</td>\n",
       "      <td>['Kids']</td>\n",
       "      <td>['Studio Ghibli']</td>\n",
       "      <td>[]</td>\n",
       "      <td>Short film shown only in the Ghibli Museum in ...</td>\n",
       "      <td>Treasure Hunting</td>\n",
       "      <td>たからさがし</td>\n",
       "      <td>1</td>\n",
       "      <td>Spring</td>\n",
       "      <td>Spring</td>\n",
       "      <td>94.0</td>\n",
       "      <td>94.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5372</th>\n",
       "      <td>Nakedyouth</td>\n",
       "      <td>ona</td>\n",
       "      <td>5.690000</td>\n",
       "      <td>-0.162739</td>\n",
       "      <td>finished_airing</td>\n",
       "      <td>-0.462556</td>\n",
       "      <td>original</td>\n",
       "      <td>-0.187413</td>\n",
       "      <td>-0.112969</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>['Boys Love', 'Romance', 'Slice of Life', 'Spo...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>['Kojiro Shishido Animation Works']</td>\n",
       "      <td>In Nakedyouth, Shishido takes us on a journey ...</td>\n",
       "      <td>Naked Youth</td>\n",
       "      <td>Nakedyouth</td>\n",
       "      <td>1</td>\n",
       "      <td>Winter</td>\n",
       "      <td>Winter</td>\n",
       "      <td>89.0</td>\n",
       "      <td>89.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15978</th>\n",
       "      <td>Yu☆Gi☆Oh! GX Tokubetsu-hen</td>\n",
       "      <td>one_shot</td>\n",
       "      <td>6.550000</td>\n",
       "      <td>-0.197994</td>\n",
       "      <td>finished</td>\n",
       "      <td>-0.462556</td>\n",
       "      <td>Missing</td>\n",
       "      <td>-0.230989</td>\n",
       "      <td>-0.114568</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>['Adventure', 'Comedy', 'Fantasy']</td>\n",
       "      <td>['School', 'Strategy Game']</td>\n",
       "      <td>['Shounen']</td>\n",
       "      <td>Kazuki Takahashi, Naoyuki Kageyama</td>\n",
       "      <td>['V-Jump']</td>\n",
       "      <td>A special one-shot released in commemoration o...</td>\n",
       "      <td>Yu-Gi-Oh! GX</td>\n",
       "      <td>遊☆戯☆王GX特別編</td>\n",
       "      <td>0</td>\n",
       "      <td>Spring</td>\n",
       "      <td>Spring</td>\n",
       "      <td>97.0</td>\n",
       "      <td>97.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    title      type     score  scored_by  \\\n",
       "13870  Souzai Saishuka no Isekai Ryokouki     manga  6.530000  -0.178503   \n",
       "8825                     Kingyo no Isshou     movie  6.583333  -0.202222   \n",
       "6178                       Takara-sagashi     movie  6.170000  -0.199299   \n",
       "5372                           Nakedyouth       ona  5.690000  -0.162739   \n",
       "15978          Yu☆Gi☆Oh! GX Tokubetsu-hen  one_shot  6.550000  -0.197994   \n",
       "\n",
       "                     status  episodes/chapters        source   members  \\\n",
       "13870  currently_publishing          -0.285907       Missing -0.185451   \n",
       "8825        finished_airing          -0.462556      original -0.236163   \n",
       "6178        finished_airing          -0.462556  picture_book -0.224216   \n",
       "5372        finished_airing          -0.462556      original -0.187413   \n",
       "15978              finished          -0.462556       Missing -0.230989   \n",
       "\n",
       "       favorites   sfw  approved  \\\n",
       "13870  -0.111569  True      True   \n",
       "8825   -0.114968  True      True   \n",
       "6178   -0.114968  True      True   \n",
       "5372   -0.112969  True      True   \n",
       "15978  -0.114568  True      True   \n",
       "\n",
       "                                                  genres  \\\n",
       "13870                           ['Adventure', 'Fantasy']   \n",
       "8825                                   ['Slice of Life']   \n",
       "6178                            ['Adventure', 'Fantasy']   \n",
       "5372   ['Boys Love', 'Romance', 'Slice of Life', 'Spo...   \n",
       "15978                 ['Adventure', 'Comedy', 'Fantasy']   \n",
       "\n",
       "                            themes demographics  \\\n",
       "13870  ['Isekai', 'Reincarnation']           []   \n",
       "8825                            []           []   \n",
       "6178                            []     ['Kids']   \n",
       "5372                            []           []   \n",
       "15978  ['School', 'Strategy Game']  ['Shounen']   \n",
       "\n",
       "                                 creators  \\\n",
       "13870                Tomozo, Masuo Kinoko   \n",
       "8825                                   []   \n",
       "6178                    ['Studio Ghibli']   \n",
       "5372                                   []   \n",
       "15978  Kazuki Takahashi, Naoyuki Kageyama   \n",
       "\n",
       "                         production_source  \\\n",
       "13870             ['AlphaPolis Web Manga']   \n",
       "8825                                    []   \n",
       "6178                                    []   \n",
       "5372   ['Kojiro Shishido Animation Works']   \n",
       "15978                           ['V-Jump']   \n",
       "\n",
       "                                                synopsis  \\\n",
       "13870  Takeru Kamishiro is an ordinary middle aged sa...   \n",
       "8825   An elementary school student named Mitsui won ...   \n",
       "6178   Short film shown only in the Ghibli Museum in ...   \n",
       "5372   In Nakedyouth, Shishido takes us on a journey ...   \n",
       "15978  A special one-shot released in commemoration o...   \n",
       "\n",
       "                            title_english title_japanese  is_anime  \\\n",
       "13870    A Gatherer's Adventure in Isekai   素材採取家の異世界旅行記         0   \n",
       "8825   Short and Happy Life of a Goldfish          金魚の一生         1   \n",
       "6178                     Treasure Hunting         たからさがし         1   \n",
       "5372                          Naked Youth     Nakedyouth         1   \n",
       "15978                        Yu-Gi-Oh! GX     遊☆戯☆王GX特別編         0   \n",
       "\n",
       "      start_season end_season  elapsed_start_time   elapsed_end_time   \n",
       "13870       Autumn    Missing                100.0          92.333333  \n",
       "8825        Winter     Winter                 76.0          76.000000  \n",
       "6178        Spring     Spring                 94.0          94.000000  \n",
       "5372        Winter     Winter                 89.0          89.000000  \n",
       "15978       Spring     Spring                 97.0          97.000000  "
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Selecting columns to normalize\n",
    "columns_to_normalize = [\"scored_by\", \"episodes/chapters\", \"members\", \"favorites\"]\n",
    "\n",
    "earliest_start_year = train['start_year'].min()\n",
    "\n",
    "# function for Normalizing the selected columns\n",
    "def Normalizing(data):\n",
    "    data_normalized = data.copy()\n",
    "    data_normalized[columns_to_normalize] = scaler.fit_transform(train[columns_to_normalize])\n",
    "\n",
    "    # Calculate the duration by subtracting the earliest start year from all years\n",
    "    data_normalized['elapsed_start_time '] = data_normalized['start_year'] - earliest_start_year\n",
    "    data_normalized['elapsed_end_time '] = data_normalized['end_year'] - earliest_start_year\n",
    "\n",
    "    data_normalized = data_normalized.drop(columns=['start_year', 'end_year'])\n",
    "\n",
    "    return data_normalized\n",
    "\n",
    "train_normalized = Normalizing(train)\n",
    "train_normalized.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization and lemmatization by keeping `{'NOUN', 'VERB', 'ADJ', 'PROPN', 'ADV'}` and replace named entity with place holder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows after filtering empty token:  19309\n"
     ]
    }
   ],
   "source": [
    "# tokenization and lemmatization\n",
    "import spacy\n",
    "\n",
    "POS_TO_KEEP = {'NOUN', 'VERB', 'ADJ', 'PROPN', 'ADV'}\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "stop_words = nlp.Defaults.stop_words\n",
    "\n",
    "def tokenization(doc):\n",
    "    \"\"\"\n",
    "    Filter out number.\n",
    "    Replace person, organization, and location entities with '<ent_type_>'\n",
    "    Return lemma if its POS is in `POS_TO_KEEP` and not in stop_words.\n",
    "    \"\"\"\n",
    "    tokens = []\n",
    "\n",
    "    for token in doc:\n",
    "        if token.is_digit:\n",
    "            # filter out numeric tokens\n",
    "            continue\n",
    "\n",
    "        if token.ent_type_ in ['PERSON', 'ORG', 'GPE']:\n",
    "            # replace person, organization, and location entities\n",
    "            tokens.append(f'<{token.ent_type_}>')\n",
    "            \n",
    "        elif token.pos_ in POS_TO_KEEP and not token.lemma_ in stop_words:\n",
    "            # return lemma if its POS is in `POS_TO_KEEP` and not in stop_words\n",
    "            tokens.append(token.lemma_)\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# use nlp.pipe for batch processing\n",
    "train['title_en_token'] = [\n",
    "    tokenization(doc) for doc in nlp.pipe(train['title_english'], batch_size=100, n_process=-1)\n",
    "]\n",
    "train['synopsis_token'] = [\n",
    "    tokenization(doc) for doc in nlp.pipe(train['synopsis'], batch_size=100, n_process=-1)\n",
    "]\n",
    "\n",
    "# filter out empty token\n",
    "train = train[train['title_en_token'].apply(lambda x: len(x) > 0) & train['synopsis_token'].apply(lambda x: len(x) > 0)] # both columns cannot be empty\n",
    "\n",
    "print('Number of rows after filtering empty token: ', len(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title_english</th>\n",
       "      <th>title_en_token</th>\n",
       "      <th>synopsis</th>\n",
       "      <th>synopsis_token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16373</th>\n",
       "      <td>Marked By King Bs</td>\n",
       "      <td>[Marked, King, Bs]</td>\n",
       "      <td>High school is hard enough without a target on your back, but that's exactly the situation Annie finds herself in when she crosses a group of the most popular kids in school. Marked by the king bee himself, the notorious Ashton Griffin, Annie becomes his newest fixation—and he is determined to make her life miserable. Now at his beck and call, Annie must stay on Ashton's good side to maintain her peaceful life and avoid becoming a social pariah. As she navigates her way through alienating social cliques, persistent old crushes, and the hot upstairs neighbor who never puts a shirt on, Annie will soon learn that there's more to being popular than meets the eye. She just wanted to live a normal life, but maybe there's no escaping these king bees.\\r\\n\\r\\n</td>\n",
       "      <td>[high, school, hard, target, exactly, situation, &lt;PERSON&gt;, find, cross, group, popular, kid, school, mark, king, bee, notorious, &lt;PERSON&gt;, &lt;PERSON&gt;, &lt;PERSON&gt;, new, fixation, determined, life, miserable, beck, &lt;PERSON&gt;, stay, &lt;ORG&gt;, good, maintain, peaceful, life, avoid, social, pariah, navigate, way, alienate, social, clique, persistent, old, crush, hot, upstairs, neighbor, shirt, &lt;PERSON&gt;, soon, learn, popular, meet, eye, want, live, normal, life, maybe, escape, king, bee]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           title_english      title_en_token  \\\n",
       "16373  Marked By King Bs  [Marked, King, Bs]   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        synopsis  \\\n",
       "16373  High school is hard enough without a target on your back, but that's exactly the situation Annie finds herself in when she crosses a group of the most popular kids in school. Marked by the king bee himself, the notorious Ashton Griffin, Annie becomes his newest fixation—and he is determined to make her life miserable. Now at his beck and call, Annie must stay on Ashton's good side to maintain her peaceful life and avoid becoming a social pariah. As she navigates her way through alienating social cliques, persistent old crushes, and the hot upstairs neighbor who never puts a shirt on, Annie will soon learn that there's more to being popular than meets the eye. She just wanted to live a normal life, but maybe there's no escaping these king bees.\\r\\n\\r\\n   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       synopsis_token  \n",
       "16373  [high, school, hard, target, exactly, situation, <PERSON>, find, cross, group, popular, kid, school, mark, king, bee, notorious, <PERSON>, <PERSON>, <PERSON>, new, fixation, determined, life, miserable, beck, <PERSON>, stay, <ORG>, good, maintain, peaceful, life, avoid, social, pariah, navigate, way, alienate, social, clique, persistent, old, crush, hot, upstairs, neighbor, shirt, <PERSON>, soon, learn, popular, meet, eye, want, live, normal, life, maybe, escape, king, bee]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# review tokenization\n",
    "with pd.option_context('display.max_colwidth', None):\n",
    "    display(train[['title_english', 'title_en_token', 'synopsis', 'synopsis_token']].sample(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since titles and synopses play different roles (titles are short and often genre-indicative, while synopses provide detailed content descriptions), we use two separate vectorizers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Titles are shorter and often contain rare, context-rich words that are crucial for capturing unique meaning, while synopses are longer and contain more common words, making them less reliant on capturing rare vocabulary. So, we use a higher `max_features` or `vector_size` for title to ensures it capture these niche terms and their relationships, while use lower for synopsis to focus on the more frequently relevant words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# initialize title tfidf\n",
    "tfidf_title = TfidfVectorizer(\n",
    "    ngram_range=(1,1),  # uni-gram\n",
    "    min_df=1,           # don't filter rare words as they are important for title\n",
    "    max_df=0.8,         # filter out very common words\n",
    ")\n",
    "\n",
    "# initialize synopsis tfidf\n",
    "tfidf_synopsis = TfidfVectorizer(\n",
    "    ngram_range=(1,2),  # uni-gram or bi-gram\n",
    "    max_features=2000,  # focus on the more frequently relevant words\n",
    "    min_df=2,           # filter out extremely rare words\n",
    "    max_df=0.8,         # filter out very common words\n",
    ")\n",
    "\n",
    "# train tfidf\n",
    "title_en_tfidf_matrix = tfidf_title.fit_transform(train['title_en_token'].apply(lambda x: \" \".join(x)))\n",
    "synopsis_tfidf_matrix = tfidf_synopsis.fit_transform(train['synopsis_token'].apply(lambda x: \" \".join(x)))\n",
    "\n",
    "# add result to train df\n",
    "train['title_en_tfidf'] = [title_en_tfidf_matrix[i] for i in range(title_en_tfidf_matrix.shape[0])]\n",
    "train['synopsis_tfidf'] = [synopsis_tfidf_matrix[i] for i in range(synopsis_tfidf_matrix.shape[0])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For word embedding, we choose **Word2Vec - Skip-gram** because it tends to capture rare words more effectively (e.g. niche anime/manga-specific vocabulary), comparing to Word2Vec - CBOW and GloVe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word embedding\n",
    "import multiprocessing\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# dynamically determine the number of CPU cores\n",
    "num_workers = multiprocessing.cpu_count()\n",
    "\n",
    "# train title skipgram model\n",
    "skipgram_model_title = Word2Vec(\n",
    "    train['title_en_token'].tolist(),\n",
    "    sg=1,               # skip-gram\n",
    "    vector_size=300,    # title use higher dim\n",
    "    window=2,           # title use smaller window size\n",
    "    min_count=1,        # titles may contain rare but important words\n",
    "    epochs=30,          # title are shorter, need more epochs to train\n",
    "    workers=num_workers,\n",
    "    seed=RANDOM_SEED\n",
    ")\n",
    "\n",
    "# train synopsis skipgram model\n",
    "skipgram_model_synopsis = Word2Vec(\n",
    "    train['synopsis_token'].tolist(),\n",
    "    sg=1,               # skip-gram\n",
    "    vector_size=150,    # synopsis use lower dim\n",
    "    window=5,           # synopsis use larger window size\n",
    "    min_count=2,        # filter out extremely rare words\n",
    "    epochs=15,\n",
    "    workers=num_workers,\n",
    "    seed=RANDOM_SEED\n",
    ")\n",
    "\n",
    "# clean tokens that does not exist in the skipgram vocab (because of `min_count`)\n",
    "model_vocab = set(skipgram_model_synopsis.wv.index_to_key)\n",
    "train['synopsis_token'] = train['synopsis_token'].apply(lambda x: [token for token in x if token in model_vocab])\n",
    "train = train[train['synopsis_token'].apply(lambda x: len(x) > 0)]  # filter out empty entry after clean tokens\n",
    "\n",
    "# apply skipgram model\n",
    "train['title_en_skipgram'] = train['title_en_token'].apply(lambda x: skipgram_model_title.wv[x])\n",
    "train['synopsis_skipgram'] = train['synopsis_token'].apply(lambda x: skipgram_model_synopsis.wv[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title_japanese</th>\n",
       "      <th>title_english</th>\n",
       "      <th>synopsis</th>\n",
       "      <th>title_en_tfidf</th>\n",
       "      <th>title_en_skipgram</th>\n",
       "      <th>synopsis_tfidf</th>\n",
       "      <th>synopsis_skipgram</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23943</th>\n",
       "      <td>愛を知った一週間</td>\n",
       "      <td>Chosen As The Frenchman's Bride</td>\n",
       "      <td>After losing her father at a young age, Jane g...</td>\n",
       "      <td>(0, 5365)\\t0.20308269236008522\\n  (0, 1298)\\...</td>\n",
       "      <td>[[-0.050672267, -0.07246053, 0.14060606, -0.11...</td>\n",
       "      <td>(0, 1990)\\t0.12745880457602696\\n  (0, 969)\\t...</td>\n",
       "      <td>[[0.37020886, 0.024892427, -0.081325725, -0.54...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20488</th>\n",
       "      <td>ふたりぽっち</td>\n",
       "      <td>The Two Daughters</td>\n",
       "      <td>Kaoru has to stay with Reiko's family, a girl ...</td>\n",
       "      <td>(0, 1793)\\t1.0</td>\n",
       "      <td>[[-0.03223555, -0.11463614, 0.18719757, -0.009...</td>\n",
       "      <td>(0, 1224)\\t0.26511441322114115\\n  (0, 523)\\t...</td>\n",
       "      <td>[[-0.05985926, 0.18823454, -0.102404416, -0.12...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12351</th>\n",
       "      <td>奪う者 奪われる者</td>\n",
       "      <td>Bereave or Bereaved</td>\n",
       "      <td>Seto Yu is a 12-year-old boy, despite being ve...</td>\n",
       "      <td>(0, 731)\\t0.7071067811865476\\n  (0, 732)\\t0....</td>\n",
       "      <td>[[-0.009359805, -0.022338165, 0.03214639, -0.0...</td>\n",
       "      <td>(0, 1968)\\t0.12348505189125379\\n  (0, 162)\\t...</td>\n",
       "      <td>[[-0.05985926, 0.18823454, -0.102404416, -0.12...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23146</th>\n",
       "      <td>まんがグリム童話 血と悦楽に溺れた実在姫君たち</td>\n",
       "      <td>Real Princess who Indulge in Blood &amp; Lust</td>\n",
       "      <td>1-2. Last Emperor no Tsuma: Enyou Kougou\\r\\n3....</td>\n",
       "      <td>(0, 5365)\\t0.5041341482361837\\n  (0, 5886)\\t...</td>\n",
       "      <td>[[-0.02228411, -0.12334833, 0.120474376, -0.08...</td>\n",
       "      <td>(0, 1224)\\t0.6939879716446256\\n  (0, 1290)\\t...</td>\n",
       "      <td>[[-0.17345911, 0.00636432, -0.08250168, -0.332...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2815</th>\n",
       "      <td>王ドロボウ JING in Seventh Heaven</td>\n",
       "      <td>Jing: King of Bandits - Seventh Heaven</td>\n",
       "      <td>Jing, the infamous King of Bandits, finds hims...</td>\n",
       "      <td>(0, 4020)\\t0.31691060798982196\\n  (0, 3364)\\...</td>\n",
       "      <td>[[-0.03958812, -0.035999063, 0.12214554, 4.222...</td>\n",
       "      <td>(0, 1968)\\t0.11052129494573408\\n  (0, 1152)\\...</td>\n",
       "      <td>[[0.051790062, 0.025802549, -0.02216094, -0.11...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     title_japanese  \\\n",
       "23943                      愛を知った一週間   \n",
       "20488                        ふたりぽっち   \n",
       "12351                     奪う者 奪われる者   \n",
       "23146       まんがグリム童話 血と悦楽に溺れた実在姫君たち   \n",
       "2815   王ドロボウ JING in Seventh Heaven   \n",
       "\n",
       "                                   title_english  \\\n",
       "23943            Chosen As The Frenchman's Bride   \n",
       "20488                          The Two Daughters   \n",
       "12351                        Bereave or Bereaved   \n",
       "23146  Real Princess who Indulge in Blood & Lust   \n",
       "2815      Jing: King of Bandits - Seventh Heaven   \n",
       "\n",
       "                                                synopsis  \\\n",
       "23943  After losing her father at a young age, Jane g...   \n",
       "20488  Kaoru has to stay with Reiko's family, a girl ...   \n",
       "12351  Seto Yu is a 12-year-old boy, despite being ve...   \n",
       "23146  1-2. Last Emperor no Tsuma: Enyou Kougou\\r\\n3....   \n",
       "2815   Jing, the infamous King of Bandits, finds hims...   \n",
       "\n",
       "                                          title_en_tfidf  \\\n",
       "23943    (0, 5365)\\t0.20308269236008522\\n  (0, 1298)\\...   \n",
       "20488                                     (0, 1793)\\t1.0   \n",
       "12351    (0, 731)\\t0.7071067811865476\\n  (0, 732)\\t0....   \n",
       "23146    (0, 5365)\\t0.5041341482361837\\n  (0, 5886)\\t...   \n",
       "2815     (0, 4020)\\t0.31691060798982196\\n  (0, 3364)\\...   \n",
       "\n",
       "                                       title_en_skipgram  \\\n",
       "23943  [[-0.050672267, -0.07246053, 0.14060606, -0.11...   \n",
       "20488  [[-0.03223555, -0.11463614, 0.18719757, -0.009...   \n",
       "12351  [[-0.009359805, -0.022338165, 0.03214639, -0.0...   \n",
       "23146  [[-0.02228411, -0.12334833, 0.120474376, -0.08...   \n",
       "2815   [[-0.03958812, -0.035999063, 0.12214554, 4.222...   \n",
       "\n",
       "                                          synopsis_tfidf  \\\n",
       "23943    (0, 1990)\\t0.12745880457602696\\n  (0, 969)\\t...   \n",
       "20488    (0, 1224)\\t0.26511441322114115\\n  (0, 523)\\t...   \n",
       "12351    (0, 1968)\\t0.12348505189125379\\n  (0, 162)\\t...   \n",
       "23146    (0, 1224)\\t0.6939879716446256\\n  (0, 1290)\\t...   \n",
       "2815     (0, 1968)\\t0.11052129494573408\\n  (0, 1152)\\...   \n",
       "\n",
       "                                       synopsis_skipgram  \n",
       "23943  [[0.37020886, 0.024892427, -0.081325725, -0.54...  \n",
       "20488  [[-0.05985926, 0.18823454, -0.102404416, -0.12...  \n",
       "12351  [[-0.05985926, 0.18823454, -0.102404416, -0.12...  \n",
       "23146  [[-0.17345911, 0.00636432, -0.08250168, -0.332...  \n",
       "2815   [[0.051790062, 0.025802549, -0.02216094, -0.11...  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[['title_japanese', 'title_english', 'synopsis', 'title_en_tfidf', 'title_en_skipgram', 'synopsis_tfidf', 'synopsis_skipgram']].sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export necessary assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['assets/skipgram_model_synopsis.joblib']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# train and test df\n",
    "joblib.dump(train, 'assets/train.joblib')\n",
    "joblib.dump(test, 'assets/test.joblib')\n",
    "\n",
    "# tfidf_matrix\n",
    "joblib.dump(title_en_tfidf_matrix, 'assets/title_en_tfidf_matrix.joblib')\n",
    "joblib.dump(synopsis_tfidf_matrix, 'assets/synopsis_tfidf_matrix.joblib')\n",
    "\n",
    "# tfidf vectorizer\n",
    "joblib.dump(tfidf_title, 'assets/tfidf_title_vectorizer.joblib')\n",
    "joblib.dump(tfidf_synopsis, 'assets/tfidf_synopsis_vectorizer.joblib')\n",
    "\n",
    "# skipgram\n",
    "joblib.dump(skipgram_model_title, 'assets/skipgram_model_title.joblib')\n",
    "joblib.dump(skipgram_model_synopsis, 'assets/skipgram_model_synopsis.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store library version\n",
    "# run every time before you commit\n",
    "!pip freeze > requirements.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env696",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
